\chapter{Intégration Monte Carlo}
\label{chap:montecarlo}

<<echo=FALSE>>=
options(width = 52)
@

\begin{objectifs}
\item Définir le principe du calcul de la valeur d'une intégrale
  définie à l'aide de nombres aléatoires.
\item Calculer la valeur d'une intégrale définie par la méthode Monte
  Carlo.
\end{objectifs}

On appelle simulation Monte Carlo (ou méthode de Monte Carlo) toute
méthode consistant à résoudre une expression mathématique à l'aide de
nombres aléatoires. Par extension, l'appellation est souvent utilisée
pour référer à toute utilisation de la simulation.

L'une des utilisations de la simulation Monte Carlo la plus répandue
est le calcul d'intégrales, principalement pour des dimensions
supérieures à un. On ne présente ici que l'idée générale.


\begin{prob-enonce}
  L'intégrale de Gauss
  \begin{equation*}
    \int e^{-x^2}\, dx
  \end{equation*}
  intervient dans plusieurs disciplines des mathématiques. En
  probabilités, elle est notamment à la base de la définition de la
  loi normale, ou loi gaussienne. Sans primitive, on l'utilise
  souvent, en calcul intégral, comme un exemple d'intégrale définie
  qu'il faut résoudre par la transformation du domaine en coordonnées
  polaires.

  On sait que
  \begin{equation*}
    G = \int_{-\infty}^{\infty} e^{-x^2}\, dx = \sqrt{\pi}.
  \end{equation*}
  Comment pourrait-on vérifier ce résultat à l'aide de la simulation?
\end{prob-enonce}


\section{Contexte}
\label{sec:montecarlo:contexte}

Supposons que l'on souhaite calculer l'intégrale
\begin{equation}
  \label{eq:montecarlo:int:depart}
  \theta = \int_a^b h(x)\, dx,
\end{equation}
mais que celle-ci est trop compliquée pour être évaluée explicitement.
On pourrait d'abord faire deux approximations de l'aire sous la
fonction $h$ à l'aide des sommes de Riemann
\begin{align*}
  R_n^- &= \sum_{k = 0}^{n - 1} h(a + k \Delta x) \Delta x \\
  \intertext{et}
  R_n^+ &= \sum_{k = 1}^n h(a + k \Delta x) \Delta x,
\end{align*}
où $\Delta x = (b - a)/n$ et $n$ est grand. Voir la
\autoref{fig:montecarlo:riemann} pour une illustration de ces deux
aires.

Par la suite, une estimation numérique de l'intégrale serait la
moyenne des deux sommes de Riemann:
\begin{equation*}
  \hat{\theta}_n = \frac{R_n^- + R_n^+}{2}.
\end{equation*}
Cette procédure devient toutefois rapidement compliquée pour les
intégrales multiples à plusieurs dimensions. Il y a éventuellement
beaucoup trop de points à évaluer.

\begin{figure}
  \centering
<<echo=FALSE, fig=TRUE, width=8, height=4>>=
par(mfrow = c(1, 2), mar = c(5, 4, 2, 2))

curve(dgamma(x, 3, 2), xlim = c(0, 2), lwd = 2, ylab="h(x)")
xx <- seq(0, 2, by = 0.2)
for (i in seq.int(length(xx) - 1))
    polygon(rep(xx[c(i, i + 1)], each = 2),
            c(0, rep(dgamma(xx[i], 3, 2), 2), 0),
            col = "lightblue")
curve(dgamma(x, 3, 2), xlim = c(0, 2), lwd = 2, add = TRUE)

curve(dgamma(x, 3, 2), xlim = c(0, 2), lwd = 2, ylab="h(x)")
xx <- seq(0, 2, by = 0.2)
for (i in seq.int(length(xx) - 1))
    polygon(rep(xx[c(i, i + 1)], each = 2),
            c(0, rep(dgamma(xx[i + 1], 3, 2), 2), 0),
            col = "lightblue")
curve(dgamma(x, 3, 2), xlim = c(0, 2), lwd = 2, add = TRUE)
@
  \caption{Approximation de l'aire sous une fonction par les deux sommes
    de Riemann $R_n^-$ (gauche) et $R_n^+$ (droite)}
  \label{fig:montecarlo:riemann}
\end{figure}


\section{Méthode Monte Carlo}
\label{sec:montecarlo:methode}

L'idée de l'intégration Monte Carlo consiste à évaluer la fonction à
intégrer en des points choisis aléatoirement, puis à s'en remettre à
la Loi des grands nombres pour estimer l'intégrale.

On exprime tout d'abord la fonction $h(x)$ sous la forme
\begin{equation*}
  h(x) = g(x) f(x),
\end{equation*}
où $f(x)$ est une densité sur $(a, b)$. Ainsi, par définition de
l'espérance:
\begin{align*}
  \theta
  &= \int_a^b g(x) f(x)\, dx \\
  &= \esp{g(X)},
\end{align*}
où $X$ est la variable aléatoire avec fonction de densité de
probabilité $f(x)$. Par la suite, si $x_1, \dots, x_n$ sont des
observations simulées de la densité $f(x)$, alors
\begin{equation}
  \label{eq:montecarlo:estimation}
  \hat{\theta}_n = \frac{1}{n} \sum_{i = 1}^n g(x_i)
\end{equation}
constitue une estimation de l'intégrale $\theta$.  Par la Loi des
grands nombres, $\hat{\theta}_n \stackrel{n \rightarrow
  \infty}{\longrightarrow} \theta$.

Par souci de simplicité et parce qu'il est facile d'obtenir un
échantillon aléatoire d'une loi uniforme, on pose en général $X \sim
U(a, b)$, soit
\begin{equation*}
  f(x) = \frac{1}{b - a}, \quad a < x < b.
\end{equation*}
Dans ce cas, on a $g(x) = (b - a) h(x)$ et l'intégrale de départ
\eqref{eq:montecarlo:int:depart} se réécrit
\begin{align}
  \label{eq:montecarlo:int:uniforme}
  \theta
  &= (b - a) \int_a^b h(x)\, \frac{1}{b - a}\, dx \\
  \notag
  &= (b - a) \esp{h(X)}.
\end{align}
L'estimation \eqref{eq:montecarlo:estimation} de l'intégrale devient
alors
\begin{equation*}
  \hat{\theta}_n = \frac{b - a}{n} \sum_{i = 1}^n h(x_i),
\end{equation*}
où $x_1, \dots, x_n$ est un échantillon aléatoire d'une loi $U(a, b)$.

On remarquera qu'il est équivalent de faire le changement de variable
\begin{equation*}
  \begin{split}
    u  &= \frac{x - a}{b - a} \\
    du &= \frac{dx}{b - a}
  \end{split}
  \quad \Leftrightarrow \quad
  \begin{split}
    x  &= a + (b - a) u \\
    dx &= (b - a)\, du
  \end{split}
\end{equation*}
dans l'intégrale~\eqref{eq:montecarlo:int:uniforme}. On obtient alors
\begin{align*}
  \theta
  &= (b - a) \int_0^1 h(a + (b - a)u) (1)\, du \\
  &= (b - a) \esp{h(a + (b - a)U)},
\end{align*}
où $U \sim U(0, 1)$. Une estimation de l'intégrale est donc
\begin{equation*}
  \hat{\theta}_n = \frac{b - a}{n} \sum_{i = 1}^n h(a + (b - a) u_i),
\end{equation*}
où $u_1, \dots, u_n$ est un échantillon aléatoire d'une loi $U(0, 1)$.
On doit utiliser la technique du changement de variable lorsque le
domaine est infini.


\begin{prob-astuce}
  La présentation ci-dessus repose sur un domaine d'intégration fini.
  Or, celui de l'intégrale à calculer est infini. Pour utiliser des
  nombres aléatoires uniformes dans la méthode Monte-Carlo, il faut
  ramener le domaine d'intégration infini à un domaine fini par le
  biais d'un changement de variable.

  Tout d'abord, par symétrie de l'intégrande,
  \begin{align*}
    G &= \int_{-\infty}^{\infty} e^{-x^2}\, dx \\
      &= 2 \int_{0}^{\infty} e^{-x^2}\, dx \\
      &= 2
        \left(
        \int_{0}^{1} e^{-x^2}\, dx +
        \int_{1}^{\infty} e^{-x^2}\, dx
        \right).
  \end{align*}

  Ensuite, plusieurs changements de variable sont possibles pour
  transformer le domaine de la seconde intégrale de droite. Nous
  choisissons le changement de variable $u = x^{-1}$. Le domaine
  d'intégration subit donc la transformation
  $(1, \infty) \mapsto (1, 0)$. Ainsi,
  \begin{align*}
    G &= 2
        \left(
        \int_{0}^{1} e^{-x^2}\, dx +
        \int_{1}^{0} e^{-u^{-2}} (-u^{-2})\, du
        \right) \\
      &= 2
        \left(
        \int_{0}^{1} e^{-x^2}\, dx +
        \int_{0}^{1} e^{-u^{-2}} u^{-2}\, du
        \right) \\
      &= 2 \int_{0}^{1} (e^{-x^2} + e^{-x^{-2}} x^{-2})\, dx.
  \end{align*}
  Exprimée ainsi, l'intégrale se prête à la méthode d'intégration
  Monte Carlo.
\end{prob-astuce}


\begin{exemple}
  \label{exemple:montecarlo:2d}
  Soit l'intégrale
  \begin{equation*}
    \theta = \int_2^5 x^{11/5} e^{-x/10}\, dx.
  \end{equation*}
  Parce que l'exposant de $x$ n'est pas un entier, on ne peut évaluer
  cette intégrale par parties. Cependant, on remarque que la fonction
  à intégrer est, à une constante près, la densité d'une loi gamma:
  \begin{align*}
    \theta
    &= \int_2^5 x^{11/5} e^{-x/10}\, dx \\
    &= \frac{\Gamma\left(\frac{16}{5}\right)}{%
      \left(\frac{1}{10}\right)^{16/5}}
    \int_2^5 \frac{\left(\frac{1}{10}\right)^{16/5}}{%
      \Gamma\left(\frac{16}{5}\right)}\, x^{16/5 - 1} e^{-x/10}\, dx \\
    &= \frac{\Gamma\left(\frac{16}{5}\right)}{%
      \left(\frac{1}{10}\right)^{16/5}}
    \left[
      G \left(5; \frac{16}{5}, \frac{1}{10} \right) -
      G \left(2; \frac{16}{5}, \frac{1}{10} \right)
    \right],
  \end{align*}
  où $G(x; \alpha, \lambda)$ est la fonction de répartition de la loi
  Gamma$(\alpha, \lambda)$. Avec R, on obtient la valeur exacte
<<echo=TRUE>>=
gamma(3.2) * diff(pgamma(c(2, 5), 3.2, 0.1)) / 0.1^3.2
@

  Pour utiliser la méthode Monte Carlo, on pose
  \begin{equation*}
    \theta = 3 \int_2^5 x^{11/5} e^{-x/10}
    \left( \frac{1}{3} \right)\, dx.
  \end{equation*}
  Si $\{x_1, \dots, x_n\}$ est un échantillon aléatoire d'une loi
  $U(2, 5)$, alors
  \begin{equation*}
    \hat{\theta}_n = \frac{3}{n} \sum_{i = 1}^n
    x_i^{11/5} e^{-x_i/10}
  \end{equation*}
  est une estimation de $\theta$. On a obtenu les résultats suivants
  avec R (voir aussi la \autoref{fig:montecarlo:2d}):
<<echo=TRUE>>=
f <- function(x) x^2.2 * exp(-x/10)
x <- runif(1e2, 2, 5)
3 * mean(f(x))
x <- runif(1e3, 2, 5)
3 * mean(f(x))
x <- runif(1e4, 2, 5)
3 * mean(f(x))
x <- runif(1e5, 2, 5)
3 * mean(f(x))
x <- runif(1e6, 2, 5)
3 * mean(f(x))
@
  \begin{figure}[t]
    \centering
<<echo=FALSE, fig=TRUE>>=
par(mfrow = c(2, 2))
f <- function(x) x^2.2 * exp(-x/10)
curve(f(x), xlim = c(2, 5), lwd = 3, main = "Vraie fonction")

x <- runif(1e2, 2, 5)
plot(x, f(x), main = "n = 100",
     pch = 21, bg = "lightblue")
x <- runif(1e3, 2, 5)
plot(x, f(x), main = "n = 1000",
     pch = 21, bg = "lightblue")
x <- runif(1e4, 2, 5)
plot(x, f(x), main = "n = 10 000",
     pch = 21, bg = "lightblue")
@
    \caption{Fonction $h(x)$ de l'\autoref{exemple:montecarlo:2d} et
      points choisis aléatoirement où la fonction est évaluée pour
      l'intégration Monte Carlo}
    \label{fig:montecarlo:2d}
  \end{figure}
  \gotorbox{Le code R pour effectuer les calculs et les graphiques
    ci-dessus est fourni à la \autoref{sec:montecarlo:code}.}%
  \qed
\end{exemple}

\begin{exemple}
  \label{exemple:montecarlo:3d}
  Soit l'intégrale
  \begin{equation*}
    \theta = \int_0^{5/4} \int_0^{5/4} \sqrt{4 - x^2 - y^2}\, dy\, dx.
  \end{equation*}
  La procédure à suivre avec les intégrales multiples est la même
  qu'avec les intégrales simples, sauf que l'on tire des points
  uniformément dans autant de dimensions que nécessaire. Ainsi, dans
  le cas présent, on pose
  \begin{align*}
    \theta
    &= \int_0^{5/4} \int_0^{5/4} \sqrt{4 - x^2 - y^2}\, dy\, dx \\
    &= \frac{25}{16} \int_0^{5/4} \int_0^{5/4} \sqrt{4 - x^2 - y^2}
    \frac{1}{\left(\frac{5}{4}\right) \left(\frac{5}{4}\right)}\, dy\, dx \\
    &= \frac{25}{16}\, \Esp{\sqrt{4 - X^2 - Y^2}},
  \end{align*}
  où $X$ et $Y$ sont des variables aléatoires indépendantes
  distribuées uniformément sur l'intervalle $(0, \frac{5}{4})$.
  Autrement dit, la densité conjointe de $X$ et $Y$ est uniforme sur
  $(0, \frac{5}{4}) \times (0, \frac{5}{4})$. Par conséquent, une
  estimation de $\theta$ calculée à partir d'un échantillon $\{(x_1,
  y_1), \dots, (x_n, y_n)\}$ tiré de cette loi conjointe est
  \begin{equation*}
    \hat{\theta}_n = \frac{25}{16 n} \sum_{i = 1}^n \sqrt{4 - x_i^2 - y_i^2}.
  \end{equation*}
  On a obtenu les résultats suivants avec R (voir aussi la
  \autoref{fig:montecarlo:3d}):
<<echo=TRUE>>=
u <- runif(1e2, 0, 1.25)
v <- runif(1e2, 0, 1.25)
mean(sqrt(4 - u^2 - v^2)) * 1.25^2
u <- runif(1e3, 0, 1.25)
v <- runif(1e3, 0, 1.25)
mean(sqrt(4 - u^2 - v^2)) * 1.25^2
u <- runif(1e4, 0, 1.25)
v <- runif(1e4, 0, 1.25)
mean(sqrt(4 - u^2 - v^2)) * 1.25^2
@
  \begin{figure}[t]
    \centering
<<echo=FALSE,fig=TRUE>>=
par(mfrow = c(2, 2), mar = c(1, 1, 2, 1))
f <- function(x, y) sqrt(4 - x^2 - y^2)
x <- seq(0, 1.25, length = 25)
y <- seq(0, 1.25, length = 25)
persp(x, y, outer(x, y, f), main = "Vraie fonction",
      zlim = c(0, 2), theta = 120, phi = 30, col = "lightblue",
      zlab = "z", ticktype = "detailed")
u <- runif(1e2, 0, 1.25)
v <- runif(1e2, 0, 1.25)
res <- persp(x, y, matrix(NA, length(x), length(y)), main = "n = 100",
             zlim = c(0, 2), theta = 120, phi = 30,
             zlab = "z", ticktype = "detailed")
points(trans3d(u, v, f(u, v), pm = res),
       pch = 21, bg = "lightblue")

u <- runif(1e3, 0, 1.25)
v <- runif(1e3, 0, 1.25)
res <- persp(x, y, matrix(NA, length(x), length(y)), main = "n = 1000",
             zlim = c(0, 2), theta = 120, phi = 30,
             zlab = "z", ticktype = "detailed")
points(trans3d(u, v, f(u, v), pm = res),
       pch = 21, bg = "lightblue")

u <- runif(1e4, 0, 1.25)
v <- runif(1e4, 0, 1.25)
res <- persp(x, y, matrix(NA, length(x), length(y)), main = "n = 10 000",
             zlim = c(0, 2), theta = 120, phi = 30,
             zlab = "z", ticktype = "detailed")
points(trans3d(u, v, f(u, v), pm = res),
       pch = 21, bg = "lightblue")
@
    \caption{Fonction $h(x)$ de l'\autoref{exemple:montecarlo:3d} et points choisis
      aléatoirement où la fonction est évaluée pour l'intégration
      Monte Carlo}
    \label{fig:montecarlo:3d}
  \end{figure}
  \gotorbox{Le code R pour effectuer les calculs et les graphiques
    ci-dessus est fourni à la \autoref{sec:montecarlo:code}.}%
  \enlargethispage{5mm}
  \qed
\end{exemple}

\begin{prob-solution}
  En posant
  \begin{align*}
    \theta
    &= 2 \int_{0}^{1} (e^{-x^2} + e^{-x^{-2}} x^{-2})\, dx \\
    &= 2 \esp{e^{-U^2} + e^{-U^{-2}} U^{-2}},
  \end{align*}
  où $U \sim U(0, 1)$, nous avons réécrit l'intégrale de Gauss comme
  l'espérance d'une transformation d'une variable aléatoire uniforme.
  Par la Loi des grands nombres, une bonne estimation de $\theta$ est
  \begin{equation*}
    \hat{\theta}_n =
    \frac{2}{n} \sum_{i=1}^{n} (e^{-u_i^2} + e^{-u_i^{-2}} u_i^{-2})
  \end{equation*}
  pour $n$ grand et où $u_1, \dots, u_n$ sont des nombres aléatoires
  distribués uniformément sur l'intervalle $(0, 1)$.

  Le calcul est ensuite très simple à faire avec R:
<<echo=FALSE>>=
options(digits = 7)
@
<<echo=TRUE>>=
u <- runif(1E5)
2 * (mean(exp(-u^2) + exp(-u^(-2)) * u^(-2)))
@
  Ce résultat est suffisamment près de $\sqrt{\pi} = 1,772454$ pour
  juger l'expérience concluante.
\end{prob-solution}


\section{Code informatique}
\label{sec:montecarlo:code}

\def\scriptfilename{montecarlo.R}

\scriptfile{\scriptfilename}
\lstinputlisting[firstline=13]{\scriptfilename}


\section{Exercices}
\label{sec:montecarlo:exercices}

\Opensolutionfile{reponses}[reponses-montecarlo]
\Opensolutionfile{solutions}[solutions-montecarlo]

\begin{Filesave}{reponses}
\bigskip
\section*{Réponses}

\end{Filesave}

\begin{Filesave}{solutions}
\section*{Chapitre \ref*{chap:montecarlo}}
\addcontentsline{toc}{section}{Chapitre \ref*{chap:montecarlo}}

\end{Filesave}

\begin{exercice}
  Évaluer l'intégrale
  \begin{displaymath}
    \int_0^1 \ln(5x + 4)\, dx
  \end{displaymath}
  exactement ainsi qu'à l'aide de l'intégration Monte Carlo. Comparer
  les réponses.
  \begin{rep}
    Valeur exacte: $1,845969$
  \end{rep}
  \begin{sol}
    On a
    \begin{align*}
      \theta
      &= \int_0^1 \ln(5u + 4)\, dx \\
      &= \esp{\ln(5U + 4)},
    \end{align*}
    où $U \sim U(0, 1)$, ou encore simplement
    \begin{displaymath}
      \theta = \esp{\ln(X)},
    \end{displaymath}
    où $X \sim U(4, 9)$. Une approximation de $\theta$ est
    \begin{displaymath}
      \hat{\theta} = \frac{1}{n} \sum_{i = 1}^n \ln(x_i),
    \end{displaymath}
    où $x_1, \dots, x_n$ est un échantillon aléatoire d'une
    distribution $U(4, 9)$. Une évaluation avec R donne
<<echo=TRUE>>=
mean(log(runif(1e6, 4, 9)))
@
  \end{sol}
\end{exercice}

\begin{exercice}
  Évaluer l'intégrale
  \begin{displaymath}
    \int_0^1 \int_0^1 e^{2xy} \ln(3x + y^2)\, dx dy
  \end{displaymath}
  à l'aide de l'intégration Monte Carlo. Comparer la réponse obtenue
  avec la vraie valeur, $1,203758$, obtenue à l'aide de Maple.
  \begin{sol}
    On pose
    \begin{align*}
      \theta
      &= \int_0^1 \int_0^1 e^{2xy} \ln(3x + y^2)\, dx dy \\
      &= \esp{e^{2XY} \ln(3X + Y^2)},
    \end{align*}
    où $X \sim U(0, 1)$, $Y \sim U(0, 1)$ et $X$ et $Y$ sont
    indépendantes. Ainsi, leur densité conjointe est uniforme sur $(0,
    1) \times (0, 1)$. Une approximation de $\theta$ est
    \begin{displaymath}
      \hat{\theta} = \frac{1}{n} \sum_{i = 1}^n
      e^{2 x_i y_i} \ln(3x_i + y_i^2)
    \end{displaymath}
    où $x_1, \dots, x_n$ et $y_1, \dots, y_n$ sont deux échantillons
    aléatoires indépendants d'une distribution $U(0, 1)$. Une
    évaluation avec R donne
<<echo=TRUE>>=
x <- runif(1e6)
y <- runif(1e6)
mean(exp(2 * x * y) * log(3 * x + y^2))
@
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit l'intégrale
  \begin{displaymath}
    \theta = \int_0^\infty x^2 \sin (\pi x) e^{-x/2}\, dx.
  \end{displaymath}
  \begin{enumerate}
  \item Évaluer cette intégrale par Monte Carlo en effectuant un
    changement de variable.
  \item Évaluer cette intégrale par Monte Carlo par échantillonnage
    direct d'une loi de probabilité appropriée.
  \end{enumerate}
  \begin{rep}
    Valeur exacte: $-0,055292$
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item Soit le changement de variable $u = e^{-x/2} \Leftrightarrow
      x = - \ln u^2$, d'où $-2 du = e^{-x/2} dx$. On a donc
      \begin{align*}
        \theta
        &= \int_0^1 2 (- \ln u^2)^2 \sin (- \pi \ln u^2)\, du \\
        &= 2 \esp{(- \ln U^2)^2 \sin (- \pi \ln U^2)},
      \end{align*}
      où $U \sim U(0, 1)$. Une estimation de $\theta$ est
      \begin{displaymath}
        \hat{\theta} = \frac{2}{n} \sum_{i = 1}^n
        (- \ln u_i^2)^2 \sin (- \pi \ln u_i^2),
      \end{displaymath}
      où $u_1, \dots, u_n$ est un échantillon aléatoire d'une
      distribution $U(0, 1)$. Une évaluation avec R donne
<<echo=TRUE>>=
u <- runif(1e6)
2 * mean((-log(u^2))^2 * sin(pi * (-log(u^2))))
@
    \item On remarque que la fonction à intégrer contient, à une
      constante près, la fonction de densité de probabilité d'une loi
      Gamma$(3, 1/2)$. Ainsi,
      \begin{align*}
        \theta
        &= 16 \int_0^\infty \sin (\pi x) \frac{(1/2)^3}{\Gamma(3)}\,
        x^2 e^{-x/2}\, dx \\
        &= 16 \esp{\sin (\pi X)},
      \end{align*}
      où $X \sim \text{Gamma}(3, 1/2)$. Une estimation de $\theta$ est
      donc
      \begin{displaymath}
        \hat{\theta} = \frac{16}{n} \sum_{i = 1}^n \sin(\pi x_i)
      \end{displaymath}
      où $x_1, \dots, x_n$ est un échantillon aléatoire d'une
      Gamma$(3, 1/2)$. Une évaluation avec R donne
<<echo=TRUE>>=
16 * mean(sin(pi * rgamma(1e6, 3, 0.5)))
@
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $X_1, \dots, X_{25}$ un échantillon aléatoire de taille $25$
  d'une distribution $N(0, 1)$ et soit $X_{(1)} \leq \dots \leq
  X_{(25)}$ les statistiques d'ordre de cet échantillon, c'est-à-dire
  les données de l'échantillon triées en ordre croissant. Estimer
  $\esp{X_{(5)}}$ par intégration Monte Carlo. Comparer la réponse
  obtenue avec la vraie espérance, $-0,90501$.
  \begin{sol}
    On peut faire l'estimation en R simplement avec les expressions
    suivantes:
<<echo=TRUE>>=
x <- replicate(10000, rnorm(25))
mean(apply(x, 2, function(x) sort(x)[5]))
@
  \end{sol}
\end{exercice}

\Closesolutionfile{reponses}
\Closesolutionfile{solutions}

\input{reponses-montecarlo}

%%% Local Variables:
%%% mode: latex
%%% TeX-engine: xetex
%%% TeX-master: "methodes-numeriques-en-actuariat_partie-1"
%%% coding: utf-8
%%% End:
